import streamlit as st
import base64
import time
import os
import json
import queue
import sounddevice as sd
import vosk
import torch
import gc
from langchain.llms import CTransformers
from langchain import PromptTemplate
import pandas as pd
import plotly.express as px
import plotly.graph_objs as go

class BlogGeneratorApp:
    def __init__(self):
        st.set_page_config(
            page_title="Content Generator",
            page_icon="üöÄ",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        self.local_css()
        self.model_path = "vosk-model-small-en-us-0.15" 
        self.q = queue.Queue()
    
        if "recognized_text" not in st.session_state:
            st.session_state["recognized_text"] = ""
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if self.device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB
            st.sidebar.success(f"‚úÖ GPU detected: {gpu_name} ({gpu_memory:.2f} GB)")
            
            if gpu_memory < 6:
                st.sidebar.info("üîß Low VRAM mode enabled automatically")
                if "low_vram_mode" not in st.session_state:
                    st.session_state["low_vram_mode"] = True
        else:
            st.sidebar.warning("‚ö†Ô∏è No GPU detected. Running on CPU.")
            if "low_vram_mode" not in st.session_state:
                st.session_state["low_vram_mode"] = False

    def local_css(self):
        st.markdown("""
        <style>
        .stApp {background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;}
        .main-container {background: rgba(255, 255, 255, 0.2); backdrop-filter: blur(10px); border-radius: 15px;}
        .stTextInput > div > div > input, .stSelectbox > div > div > select {background: rgba(255, 255, 255, 0.1); color: white;}
        .stButton > button {background: linear-gradient(45deg, #FF6B6B, #4ECDC4); color: white; border-radius: 15px;}
        .generated-text {background: rgba(255, 255, 255, 0.1); border-radius: 15px; padding: 20px;}
        .gpu-info {background: rgba(0, 0, 0, 0.2); padding: 10px; border-radius: 10px; margin-bottom: 15px;}
        </style>
        """, unsafe_allow_html=True)

    def callback(self, indata, frames, time, status):
        if status:
            print(status, flush=True)
        self.q.put(bytes(indata))

    def speech_to_text(self):
        if not os.path.exists(self.model_path):
            st.error("Vosk model not found! Place 'vosk-model-small-en-us-0.15' in the project directory.")
            return ""
        
        model = vosk.Model(self.model_path)
        recognizer = vosk.KaldiRecognizer(model, 16000)
        recognizer.SetWords(True)
        
        with sd.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16', channels=1, callback=self.callback):
            st.info("Listening... Speak now!")
            while True:
                data = self.q.get()
                if recognizer.AcceptWaveform(data):
                    result = json.loads(recognizer.Result())
                    return result.get("text", "")

    def clear_cuda_cache(self):
        """Explicitly clear CUDA cache to free up GPU memory"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            gc.collect()

    def generate_llama_blog(self, input_text, no_words, blog_style, temperature=0.7, max_tokens=512, low_vram_mode=False):
        try:
            progress_bar = st.progress(0)
            for i in range(100):
                time.sleep(0.01)
                progress_bar.progress(i + 1)
            
        
            self.clear_cuda_cache()
            
            if low_vram_mode:
                gpu_config = {
                    'max_new_tokens': max_tokens,
                    'temperature': temperature,
                    'gpu_layers': 16, 
                    'context_length': 1024, 
                    'batch_size': 1, 
                    'thread_count': 4  
                }
            else:
                gpu_config = {
                    'max_new_tokens': max_tokens, 
                    'temperature': temperature,
                    'gpu_layers': 32,  
                    'context_length': 2048  
                }
            
            if self.device == "cpu":
                gpu_config.pop('gpu_layers', None)
            
            model_path = 'llama-2-7b-chat.ggmlv3.q4_K_M.bin' if low_vram_mode else 'llama-2-7b-chat.ggmlv3.q8_0.bin'
            
            llm = CTransformers(
                model=model_path,
                model_type='llama',
                config=gpu_config
            )
            
            template = """
            Write a blog for {blog_style} job profile for a topic {input_text}
            within {no_words} words.
            """
            prompt = PromptTemplate(
                input_variables=["blog_style", "input_text", 'no_words'],
                template=template
            )
            
            response = llm(prompt.format(
                blog_style=blog_style,
                input_text=input_text,
                no_words=no_words
            ))
            
            self.clear_cuda_cache()
            
            return response
        except Exception as e:
            st.error(f"Error generating blog: {str(e)}")
            self.clear_cuda_cache()
            return None

    def create_blog_metrics_visualization(self):
        metrics_data = {'Blog Style': ['Technical', 'Professional', 'Casual', 'Academic', 'Creative'],
                        'Average Words': [350, 400, 300, 450, 250],
                        'Popularity': [85, 90, 75, 65, 95]}
        df = pd.DataFrame(metrics_data)
        col1, col2 = st.columns(2)
        with col1:
            fig1 = px.bar(df, x='Blog Style', y='Average Words', title='Average Blog Length by Style', color='Blog Style')
            st.plotly_chart(fig1, use_container_width=True)
        with col2:
            fig2 = go.Figure(data=go.Scatterpolar(r=df['Popularity'], theta=df['Blog Style'], fill='toself'))
            fig2.update_layout(title='Blog Style Popularity')
            st.plotly_chart(fig2, use_container_width=True)

    def get_download_link(self, text, filename):
        b64 = base64.b64encode(text.encode()).decode()
        return f'<a href="data:file/txt;base64,{b64}" download="{filename}">üì• Download Blog</a>'

    def display_gpu_info(self):
        if self.device == "cuda":
            st.markdown('<div class="gpu-info">', unsafe_allow_html=True)
            st.subheader("üñ•Ô∏è GPU Information")
            
            cols = st.columns(3)
            with cols[0]:
                st.metric("GPU Model", torch.cuda.get_device_name(0))
            with cols[1]:
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
                st.metric("Memory Used", f"{memory_allocated:.2f} MB")
            with cols[2]:
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
                memory_percent = (memory_allocated / memory_total) * 100
                st.metric("Memory Usage", f"{memory_percent:.1f}%")
                
            st.markdown('</div>', unsafe_allow_html=True)

    def run(self):
        st.markdown("""<h1 style='text-align: center; color: white;'>üöÄ GPU-Accelerated AI Content Generator</h1>""", unsafe_allow_html=True)
        st.markdown('<div class="main-container">', unsafe_allow_html=True)
        
        self.display_gpu_info()
        
        with st.sidebar:
            st.header("üß† Blog Generation Settings")
            
            if self.device == "cuda":
                low_vram_mode = st.checkbox("Low VRAM Mode (4GB GPUs)", 
                                           value=st.session_state.get("low_vram_mode", False),
                                           help="Enable for GTX 1650, RTX 3050 or other 4GB VRAM GPUs")
                st.session_state["low_vram_mode"] = low_vram_mode
                
                if low_vram_mode:
                    st.info("‚úÖ Using 4-bit quantized model and reduced context")
                    model_choice = "llama-2-7b-chat.ggmlv3.q4_K_M.bin"
                else:
                    model_choice = st.selectbox("Model Precision", 
                                              ["llama-2-7b-chat.ggmlv3.q4_K_M.bin", 
                                               "llama-2-7b-chat.ggmlv3.q5_K_M.bin",
                                               "llama-2-7b-chat.ggmlv3.q8_0.bin"],
                                              index=0,
                                              help="Lower precision (q4) uses less VRAM")
            else:
                low_vram_mode = False
                model_choice = "llama-2-7b-chat.ggmlv3.q4_K_M.bin"
                
            advanced_mode = st.checkbox("Advanced Mode")
            if advanced_mode:
                temperature = st.slider("Temperature (Creativity)", 0.0, 1.0, 0.7, 0.1)
                max_tokens = st.number_input("Maximum Tokens", 100, 1024 if low_vram_mode else 2048, 
                                            512 if not low_vram_mode else 256)
                
                if self.device == "cuda" and not low_vram_mode:
                    gpu_layers = st.slider("GPU Layers", 1, 32, 16, 
                                         help="More layers use more VRAM but process faster")
                else:
                    gpu_layers = 16 if self.device == "cuda" else 0
            else:
                temperature = 0.7
                max_tokens = 256 if low_vram_mode else 512
                gpu_layers = 16 if self.device == "cuda" else 0

        col1, col2 = st.columns(2)
        
        with col1:
            input_text = st.text_input("Content Topic", st.session_state["recognized_text"])
            
            if st.button("üé§ Speak Topic"):
                recognized_text = self.speech_to_text()
                st.session_state["recognized_text"] = recognized_text  
                st.experimental_rerun()  
        
        with col2:
            blog_styles = {'üñ•Ô∏è Technical': 'technical', 'üíº Professional': 'professional', 'üòé Casual': 'casual', 'üéì Academic': 'academic', 'üé® Creative': 'creative'}
            blog_style = st.selectbox("Blog Style", list(blog_styles.keys()))
            blog_style = blog_styles[blog_style]

        no_words = st.slider("üìè Word Count", 100, 1000, 300, 50)

        if st.button("‚ú® Generate Magical Content ‚ú®"):
            if not input_text:
                st.warning("üö® Please enter a topic")
            else:
                start_time = time.time()
                
                generated_blog = self.generate_llama_blog(
                    input_text, 
                    no_words, 
                    blog_style, 
                    temperature=temperature, 
                    max_tokens=max_tokens,
                    low_vram_mode=low_vram_mode
                )
                
                generation_time = time.time() - start_time
                
                if generated_blog:
                    st.markdown('<div class="generated-text">', unsafe_allow_html=True)
                    st.write(generated_blog)
                    st.markdown('</div>', unsafe_allow_html=True)
                    st.success(f"‚úÖ Content generated in {generation_time:.2f} seconds")
                    st.markdown(self.get_download_link(generated_blog, f"{input_text.replace(' ', '_')}_blog.txt"), unsafe_allow_html=True)

        if self.device == "cuda":
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            memory_percent = (memory_allocated / memory_total) * 100
            st.sidebar.metric("Current VRAM Usage", f"{memory_percent:.1f}%", f"{memory_allocated:.1f} MB")

        st.markdown('</div>', unsafe_allow_html=True)
        
        if not low_vram_mode or self.device == "cpu":
            st.header("üìä Content Generation Insights")
            self.create_blog_metrics_visualization()

def main():
    app = BlogGeneratorApp()
    app.run()

if __name__ == "__main__":
    main()
